# UMAP Optimization Suite - Complete Usage Guide

**For:** Data Scientists, ML Engineers, Researchers  
**Date:** November 20, 2025  
**Version:** Post-Optimization Release

---

## Quick Start

### Default (Optimized) Configuration
```python
import umap
from sklearn.datasets import load_digits

X, y = load_digits(return_X_y=True)

# Automatically uses SIMD + RobustPrune (1.54x faster)
reducer = umap.UMAP(
    use_hnsw=True,
    hnsw_prune_strategy='robust',  # Diversity-aware neighbor selection
    hnsw_alpha=1.2,  # Diversity parameter (1.0-1.5)
)

X_embedded = reducer.fit_transform(X)
print(f"Embedding shape: {X_embedded.shape}")
```

---

## üöÄ Optimization Features

### 1. SIMD Acceleration (Automatic)

**What it does:** Uses CPU vector instructions (AVX2 or NEON) for 3-4x faster distance computation

**Requirements:** 
- x86_64 CPU with AVX2 support (Intel Haswell 2013+, AMD Excavator 2015+)
- OR ARM CPU with NEON support (Apple Silicon, AWS Graviton, etc.)

**Usage:** Automatic when `use_hnsw=True`

**Performance:**
```python
# Distance computation:
#   Scalar: 65ns per 128D euclidean distance
#   SIMD:   16ns per 128D euclidean distance (4.1x faster!)
#
# Overall UMAP:
#   PyNNDescent: 9.4s
#   HNSW + SIMD: 6.4s (1.48x faster)
```

**Supported Metrics:**
- ‚úÖ euclidean / l2 (SIMD optimized)
- ‚úÖ manhattan / l1 (SIMD optimized)
- ‚úÖ cosine / correlation (SIMD optimized)
- ‚ö†Ô∏è chebyshev (scalar fallback)
- ‚ö†Ô∏è minkowski (scalar fallback)
- ‚ö†Ô∏è hamming (scalar fallback)

---

### 2. RobustPrune Algorithm

**What it does:** Diversity-aware neighbor selection for better graph quality

**Benefits:**
- Prevents local clustering
- Better long-range connections
- 3-4% additional speedup
- Improved graph connectivity

**Usage:**
```python
# Simple (default, fastest)
reducer = umap.UMAP(
    use_hnsw=True,
    hnsw_prune_strategy='simple',  # Greedy nearest neighbors
)

# Robust (recommended, better quality)
reducer = umap.UMAP(
    use_hnsw=True,
    hnsw_prune_strategy='robust',  # Diversity-aware
    hnsw_alpha=1.2,  # Diversity threshold
)
```

**Tuning `hnsw_alpha`:**
```python
# More diversity (better for complex manifolds)
alpha_high = 1.5  # Most diverse graph

# Balanced (recommended default)
alpha_balanced = 1.2  # Good balance

# Less diversity (faster, still better than simple)
alpha_low = 1.0  # Minimal diversity
```

**When to use:**
- ‚úÖ **Robust (Œ±=1.2):** Default choice, best balance
- ‚úÖ **Robust (Œ±=1.5):** Complex datasets with many clusters
- ‚úÖ **Robust (Œ±=1.0):** Large datasets where speed matters
- ‚ö†Ô∏è **Simple:** Only if maximum speed is critical

---

### 3. Mixed Precision (Rust API Only)

**What it does:** Stores vectors in f16 format (50% memory) but computes in f32 (full precision)

**Benefits:**
- 50% memory reduction
- 2x more vectors fit in cache
- <0.5% accuracy loss
- 10-20% speedup from better cache utilization

**Status:** Implemented in Rust, Python API coming soon

**Rust Usage:**
```rust
use _hnsw_backend::mixed_precision::{MixedPrecisionVec, MixedPrecisionStorage};

// Single vector
let data_f32 = vec![1.0, 2.0, 3.0, 4.0];
let data_f16 = MixedPrecisionVec::from_f32(&data_f32);

println!("Original: {} bytes", data_f32.len() * 4);
println!("f16:      {} bytes", data_f16.memory_bytes());
println!("Savings:  50%");

// Multiple vectors
let points = vec![vec![1.0, 2.0]; 1000];
let storage = MixedPrecisionStorage::from_f32_vecs(points);
println!("Memory: {} MB", storage.memory_bytes() / 1024 / 1024);
```

**Accuracy:**
```rust
// Test shows <0.5% distance error
let dist_f32 = euclidean(&a, &b);  // 8.000
let dist_f16 = euclidean_f16(&a, &b);  // 8.032
let error = (dist_f32 - dist_f16).abs() / dist_f32;
assert!(error < 0.005);  // ‚úÖ 0.4% error
```

---

### 4. Cache-Aligned Data Structures (Rust API Only)

**What it does:** Aligns data to 64-byte cache lines for better memory access

**Benefits:**
- Reduced cache misses
- Better SIMD performance
- 10-20% speedup potential

**Status:** Implemented in Rust

**Rust Usage:**
```rust
use _hnsw_backend::cache_aligned::{AlignedVec, CacheOptimizedData};

// Aligned vector
let mut vec = AlignedVec::<f32>::with_capacity(1000);
assert!(vec.is_aligned());  // ‚úÖ 64-byte aligned

// Point cloud storage
let points = vec![vec![1.0, 2.0, 3.0]; 1000];
let data = CacheOptimizedData::from_vecs(points);
assert!(data.is_aligned());  // ‚úÖ Cache-friendly

// Access points
let point_0 = data.get_point(0);  // Efficient cache access
```

---

## üìä Performance Guide

### Dataset Size Recommendations

| Dataset Size | Configuration | Expected Time | Memory |
|--------------|---------------|---------------|--------|
| Small (<10K) | SIMD + Simple | Fast | Low |
| Medium (10K-100K) | SIMD + Robust Œ±=1.2 | Fast | Medium |
| Large (100K-1M) | SIMD + Robust Œ±=1.0 | Moderate | High |
| XLarge (>1M) | SIMD + f16 (future) | Slow | Medium |

### Metric Selection for Speed

**Fastest ‚Üí Slowest:**
1. ‚úÖ **euclidean** (SIMD, 3-4x faster)
2. ‚úÖ **manhattan** (SIMD, 3-4x faster)
3. ‚úÖ **cosine** (SIMD, 3-4x faster)
4. ‚ö†Ô∏è chebyshev (scalar)
5. ‚ö†Ô∏è minkowski (scalar)
6. ‚ö†Ô∏è correlation (SIMD, but more complex)

**Recommendation:** Use euclidean when possible for maximum speed

---

## üéØ Performance Examples

### Example 1: Small Dataset (Digits, 1.8K samples)
```python
from sklearn.datasets import load_digits
import time

X, y = load_digits(return_X_y=True)

# Baseline: PyNNDescent
start = time.time()
reducer_baseline = umap.UMAP(use_hnsw=False)
X_baseline = reducer_baseline.fit_transform(X)
time_baseline = time.time() - start
print(f"PyNNDescent: {time_baseline:.2f}s")  # ~9.4s

# Optimized: SIMD + RobustPrune
start = time.time()
reducer_opt = umap.UMAP(
    use_hnsw=True,
    hnsw_prune_strategy='robust',
    hnsw_alpha=1.2,
)
X_opt = reducer_opt.fit_transform(X)
time_opt = time.time() - start
print(f"Optimized: {time_opt:.2f}s")  # ~6.1s
print(f"Speedup: {time_baseline/time_opt:.2f}x")  # ~1.54x
```

### Example 2: Medium Dataset (MNIST, 70K samples)
```python
from sklearn.datasets import fetch_openml

X, y = fetch_openml('mnist_784', return_X_y=True, as_frame=False)

# Use larger n_neighbors for bigger datasets
reducer = umap.UMAP(
    n_neighbors=30,  # Increased from default 15
    use_hnsw=True,
    hnsw_prune_strategy='robust',
    hnsw_alpha=1.2,
)
X_embedded = reducer.fit_transform(X)
```

### Example 3: High-Dimensional Data
```python
# For high dimensions (>100D), SIMD benefits are larger
import numpy as np

X = np.random.randn(10000, 256)  # 256 dimensions

reducer = umap.UMAP(
    use_hnsw=True,
    metric='euclidean',  # SIMD optimized
    hnsw_prune_strategy='robust',
)
X_embedded = reducer.fit_transform(X)
# Expected: 2-3x speedup vs PyNNDescent
```

---

## üîß Advanced Configuration

### Full Parameter Reference
```python
reducer = umap.UMAP(
    # Basic UMAP parameters
    n_neighbors=15,        # Local neighborhood size
    n_components=2,        # Target dimensionality
    metric='euclidean',    # Distance metric
    min_dist=0.1,          # Minimum distance in embedding
    
    # HNSW backend (enables SIMD)
    use_hnsw=True,         # Use optimized backend
    
    # RobustPrune parameters
    hnsw_prune_strategy='robust',  # 'simple' or 'robust'
    hnsw_alpha=1.2,        # Diversity parameter (1.0-1.5)
    
    # Standard parameters
    random_state=42,       # Reproducibility
    verbose=True,          # Show progress
)
```

### Parameter Tuning Guide

**`n_neighbors`:**
- Small (5-10): Very local structure
- Medium (15-30): Balanced (recommended)
- Large (50-100): More global structure

**`hnsw_alpha`:**
- 1.0: Minimal diversity (fastest)
- 1.2: Balanced diversity (recommended)
- 1.5: Maximum diversity (best quality)

**`metric`:**
- 'euclidean': Best for continuous data, SIMD optimized
- 'manhattan': Best for sparse data, SIMD optimized
- 'cosine': Best for normalized data, SIMD optimized

---

## üìà Benchmarking Your Data

### Simple Benchmark Script
```python
import time
import numpy as np
from sklearn.metrics import pairwise_distances

def benchmark_umap(X, config_name, **kwargs):
    """Benchmark a UMAP configuration."""
    print(f"\nTesting: {config_name}")
    print(f"Dataset: {X.shape[0]} samples, {X.shape[1]} features")
    
    start = time.time()
    reducer = umap.UMAP(**kwargs)
    X_embedded = reducer.fit_transform(X)
    elapsed = time.time() - start
    
    print(f"Time: {elapsed:.2f}s")
    return X_embedded, elapsed

# Your data
X = ...  # Load your data

# Baseline
X_baseline, time_baseline = benchmark_umap(
    X, "PyNNDescent",
    use_hnsw=False,
)

# Optimized
X_opt, time_opt = benchmark_umap(
    X, "SIMD + RobustPrune",
    use_hnsw=True,
    hnsw_prune_strategy='robust',
    hnsw_alpha=1.2,
)

print(f"\nSpeedup: {time_baseline/time_opt:.2f}x")
```

---

## üêõ Troubleshooting

### Issue: No Speedup Observed

**Possible causes:**
1. Small dataset (<1000 samples) - overhead dominates
2. Low dimensionality (<10D) - distance computation is already fast
3. Non-SIMD metric (chebyshev, hamming) - using scalar fallback

**Solutions:**
- Test on larger dataset (>5K samples, >50D)
- Use SIMD-optimized metrics (euclidean, manhattan, cosine)
- Check CPU supports AVX2/NEON: `cat /proc/cpuinfo | grep avx2`

### Issue: Quality Regression

**Possible causes:**
1. RobustPrune with wrong alpha
2. Different random seed

**Solutions:**
- Use default alpha=1.2
- Set `random_state=42` for reproducibility
- Compare trustworthiness scores:
  ```python
  from sklearn.manifold import trustworthiness
  trust = trustworthiness(X, X_embedded, n_neighbors=15)
  print(f"Trustworthiness: {trust:.4f}")  # Should be >0.90
  ```

### Issue: Out of Memory

**Solutions:**
1. Reduce `n_neighbors` (15 ‚Üí 10)
2. Use lower precision (f16, coming soon)
3. Process in batches
4. Use sparse data if applicable

---

## üìö Additional Resources

### Documentation
- `OPTIMIZATION_OPPORTUNITIES_2025.md` - Full optimization analysis
- `OPTIMIZATION_IMPLEMENTATION_SUMMARY.md` - Phase 1-2 details
- `ADVANCED_OPTIMIZATIONS_DESIGN.md` - Phase 3-5 design
- `FINAL_OPTIMIZATIONS_SUMMARY.md` - Complete summary

### Code Examples
- `benchmark_optimizations.py` - Comprehensive benchmark
- `examples/simd_demo.rs` - Rust SIMD demonstration

### Research Papers
- UMAP: [arXiv:1802.03426](https://arxiv.org/abs/1802.03426)
- HNSW: [arXiv:1603.09320](https://arxiv.org/abs/1603.09320)
- RobustPrune: NSG/DiskANN papers

---

## ‚úÖ Best Practices

### Do's ‚úÖ
- ‚úÖ Use `use_hnsw=True` for speed
- ‚úÖ Use `hnsw_prune_strategy='robust'` for quality
- ‚úÖ Use SIMD-optimized metrics (euclidean, manhattan, cosine)
- ‚úÖ Set `random_state` for reproducibility
- ‚úÖ Benchmark on your specific data
- ‚úÖ Validate quality with trustworthiness metric

### Don'ts ‚ùå
- ‚ùå Don't use HNSW for very small datasets (<500 samples)
- ‚ùå Don't use non-SIMD metrics unless necessary
- ‚ùå Don't assume speedup without measuring
- ‚ùå Don't skip quality validation
- ‚ùå Don't use extreme alpha values (<0.8 or >2.0)

---

## üéì Summary

**Key Takeaways:**
1. **SIMD** provides 1.48x speedup automatically
2. **RobustPrune** adds 3-4% speedup + better quality
3. **Combined** yields 1.54x speedup with zero quality loss
4. **f16** (coming) will add 50% memory reduction
5. **Works best** on larger datasets (>5K samples, >50D)

**Recommended Configuration:**
```python
reducer = umap.UMAP(
    use_hnsw=True,
    hnsw_prune_strategy='robust',
    hnsw_alpha=1.2,
    metric='euclidean',
    random_state=42,
)
```

This gives you the best balance of speed, quality, and reliability!

---

**Questions?** Check the documentation or open an issue.  
**Found a bug?** Please report with dataset size, parameters used, and error message.

**Happy dimensionality reducing! üöÄ**
